\documentclass[serif]{beamer}  % for 4:3 ratio

% Encoding & packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage{hyperref}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,listings,stackengine}
\usepackage{UoWstyle}


\author{
    Giovanni Billo
    \and  
    Andrea Suklan
    \and  
    Carlos Velázquez Fernández
}
\title{Analysis of RL Algorithms for a Simulated Hill Climb Racing Agent}
\date{\small July 28, 2025}

% Custom commands and colors
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{RGB}{153,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

% Code listing setup
\lstset{
    basicstyle=\ttfamily\tiny,
    keywordstyle=\bfseries\color{deepgreen},
    emphstyle=\ttfamily\color{deepred},
    stringstyle=\color{deepblue},
    numbers=left,
    numberstyle=\tiny\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}

\begin{frame}
    \vfill
    \begin{center}
        \includegraphics[keepaspectratio, scale=0.15]{images/logo.jpg}
        \vspace{1cm}
        \begin{beamercolorbox}[wd=\textwidth,center,rounded=true]{title}
            \textbf{Analysis of RL Algorithms for \\ a Simulated Hill Climb Racing Agent}
        \end{beamercolorbox}
        \vspace{1cm}
        {July 28, 2025}
    \end{center}
    \vfill
\end{frame} 

\begin{frame}    
\tableofcontents[sectionstyle=show,
subsectionstyle=show/shaded/hide,
subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Problem Definition}

    \begin{frame}{Markov Decision Process}

    A MDP is a \textbf{stochastic model for sequential decision making} defined by a tuple: $$(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$$.

    \begin{figure}
            \centering
            \includegraphics[width=0.65\linewidth]{images/md.png}
    \end{figure}
        
    \end{frame}

    \begin{frame}{Reward Function ($\mathcal{R}$)}
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{l r}
            \toprule
            \textbf{Event} & \textbf{Value} \\
            \midrule
            Forward Progress (per meter) & +5.0 \\
            Coin Collection & +20.0 \\
            Air Time (per second) & +5.0 \\
            Time Penalty (per step) & -0.1 \\
            Crash (Episode End) & -50.0 \\
            \bottomrule
        \end{tabular}
    \end{frame}

    \begin{frame}{Policy ($\pi$) and Discount Factor ($\gamma$)}
     The agent's goal is to learn an optimal \textbf{policy ($\pi^*$)}.
    
    \begin{itemize}
        \item In this project, the policy is approximated by a \textbf{neural network} due to the high-dimensional, continuous state space.
    \end{itemize}
        
    Future rewards are weighted by the \textbf{discount factor ($\gamma$)}.
    
    \begin{itemize}
        \item It balances the importance of immediate versus long-term rewards.
        \item We chose a high value of \textbf{$\gamma = 0.99$} to create a "far-sighted" agent and encourages long-term returns.
    \end{itemize}
        
    \end{frame}

    \begin{frame}{State Space ($\mathcal{S}$)}
        \begin{itemize}
            \item The agent has perfect knowledge of the state, leading it to a \textbf{perfect observability} scenario.
        \end{itemize}
    
        \begin{figure}
            \centering
            \includegraphics[width=0.8\linewidth]{images/states.jpg}
        \end{figure}
    \end{frame}

    \begin{frame}{Action Space ($\mathcal{A}$)}
        \begin{itemize}
            \item The agent does \textbf{not} have prior knowledge of this model, this puts it in a \textbf{model-free} context.
        \end{itemize}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\linewidth]{images/action_space.jpg}
        \end{figure}
    \end{frame}

    \begin{frame}{Problem Classification}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\linewidth]{images/knowledge_vs_observability.png}
        \end{figure}
    \end{frame}

\section{Deep Q-Network}

\begin{frame}{Characteristics of DQN}

    DQN combines the principles of \textbf{deep neural networks} with \textbf{Q-learning}.
    \vspace{1cm}
    \begin{itemize}
        \item \textbf{Off-policy:} learning from actions taken by different policies.
        \vspace{0.5cm}
        \item \textbf{Offline:} it collects a batch of experiences.
    \end{itemize}
\end{frame}

\begin{frame}{DQN Training}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{images/experience_replay_buffer.png}
    \end{figure}
    \[L(\theta) = \left( \underbrace{r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a'; \theta)}_{\text{Target}} - \underbrace{Q(s, a; \theta)}_{\text{Prediction}} \right)^2\]
\end{frame}

\begin{frame}{DQN Algorithm}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{images/DQN_diagram.png}
    \end{figure}
    \begin{itemize}
        \item Semi-Gradient Method: $\hat{q}(s, a, \mathbf{w}) \approx q_{\star}(s, a), \mathbf{w} \in \mathbb{R}$ 
        \item Uses \textit{Replay Buffer} to tackle the \textit{moving target problem}.  
    \end{itemize}
\end{frame}

\begin{frame}{More specifically}
    \textbf{Weight update in DQN}
    $$
    \mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ R + \gamma \max_{A'} \hat{q}(S', A', \mathbf{w}^{-}) - \hat{q}(S, A, \mathbf{w}) \right] \nabla \hat{q}(S, A, \mathbf{w})
    $$
\end{frame}

\section{Expected SARSA}

\begin{frame}{Characteristics of Expected SARSA}
    Expected SARSA combines the principles of statistical expectation with the on-policy learning algorithm SARSA.
    $$(s, a, r, s', a')$$
    \begin{itemize}
        \item \textbf{On-policy:} the update is based on the expected value according to the policy being followed.
        \vspace{0.5cm}
        \item \textbf{Online:} An update after each single step.
    \end{itemize}
\end{frame}

\begin{frame}{From SARSA to Expected SARSA}
    \begin{itemize}
        \item \textbf{SARSA}
        $$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]$$
        \item \textbf{Expected SARSA}
        $$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \mathbb{E}[Q(s', a')] - Q(s, a) \right]$$
        $$\underbrace{\mathbb{E}[Q(s', a')]}_{\text{Expected value over all possible next actions}} = \sum_{a' \in \mathcal{A}} \pi(a'|s') Q(s', a')$$
    \end{itemize}
\end{frame}

\begin{frame}{Loss}
$$L(\theta) = \left( \underbrace{\left( r + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') Q(s', a'; \theta^-) \right)}_{\text{Target}} - \underbrace{Q(s, a; \theta)}_{\text{Prediction}} \right)^2$$
\end{frame}

\section{Proximal Policy Optimization}

\begin{frame}{PPO Algorithm}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{images/PPO_diagram.png}
    \end{figure}
\end{frame}

\section{Results}

\begin{frame}{DQN - Loss vs Reward}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{plots/dqn_value_loss_curve.png}
        \includegraphics[width=\linewidth]{plots/dqn_rewardcurve.png}
    \end{figure}
\end{frame}

\begin{frame}{PPO - Loss vs Reward}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{plots/ppo_value_loss_curve.png}
        \includegraphics[width=\linewidth]{plots/ppo_rewardcurve.png}
    \end{figure}
\end{frame}

\begin{frame}{PPO - Entropy}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{plots/ppo_entropy_curve.png}
    \end{figure}
\end{frame}

\begin{frame}{SARSA - Loss vs Reward}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{plots/sarsa_value_loss_curve.png}
        \includegraphics[width=\linewidth]{plots/sarsa_rewardcurve.png}
    \end{figure}
\end{frame}

\begin{frame}{All vs All - Loss}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{plots/all_value_loss_curve.png}
    \end{figure}
\end{frame}

\begin{frame}{All vs All - Reward}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{plots/all_reward_curve.png}
    \end{figure}
\end{frame}


\begin{frame}
\centering
{\Huge Thank you!}
\end{frame}



\end{document}

